{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bdmZOdkETmg980NB--MeP8F0nVYqcFxL","timestamp":1696804864864}],"authorship_tag":"ABX9TyPL40Ulogyaf3o2BW4eU3ko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Scraping (part 2) and Text Analysis**"],"metadata":{"id":"QHsUOUNeIvea"}},{"cell_type":"markdown","source":["Last week, as you may remember, we worked on web-scraping. As you may remember, we did two types of scraping, to extract data from the internet. First, we directly scraped data from a wikipedia page; after that, we talked about getting data from apis.\n","\n","Today we're going to finish our work on \"scraping\" by playing with the reddit tool we spoke about, that's built on top of the api. Then we'll start learning about the types of text analysis we can do, once we're done \"scraping"],"metadata":{"id":"FN4HrRh_dLVg"}},{"cell_type":"markdown","source":["**SCRAPING PART 2**"],"metadata":{"id":"dH-skwhodjbe"}},{"cell_type":"markdown","source":["First' let's review a few things from last week. Just as an overview, you don't need to remember in too much detail.\n","\n","First, we used a package called pandas to play with \"dataframes\" which were like spreadsheets. below, see a review of code that we used to \"import\" or download the pandas package, and then code we used to create a dataframe called data1. We then wrote code to pick out an item in this dataframe, and to \"loop\" through one column of a dataframe."],"metadata":{"id":"EalMhIeXdt1i"}},{"cell_type":"code","source":["#import pandas\n","import pandas as pd"],"metadata":{"id":"ELAAndsOdsUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a dataframe called data1\n","d = {'col1': [1, 2], 'col2': [3, 4]}\n","data1 = pd.DataFrame(data=d)\n","print(data1)"],"metadata":{"id":"7_19DvxweFxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#picking out an item in col1 of the dataframe, row 1.  Note that the first row is numbered row 0, so when we pick out the item in row 1, it's actially in the second row\n","\n","data1[\"col2\"][1]"],"metadata":{"id":"E3ObMeE-efQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#loop through all the items in all of the rows of the first column of the dataframe, and print the items (in range we put 2 because there are two rows)\n","\n","for i in range(2):\n","  print(data1[\"col2\"][i])"],"metadata":{"id":"J2z281jkesay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also learned how to upload a spreadsheet file into google colab, like this:"],"metadata":{"id":"PZR_guj2e4-i"}},{"cell_type":"code","source":["#import your file\n","\n","from google.colab import files\n","uploaded = files.upload()\n","print(uploaded)\n","\n"],"metadata":{"id":"AEphQb70fCVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#put the file in a pandas dataframe by inserting its name, which should have printed out above, into the spot below.\n","\n","pd.read_excel('FILENAMEGOESHERE.xlsx')"],"metadata":{"id":"5U6YWXXHfmdI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OK now that we've reviewed that, let's go get spreadsheets of data from the reddit api using an app built for us to do so.\n","\n","Find the app here:\n","\n","https://smm.ncsa.illinois.edu/"],"metadata":{"id":"CjNknLrLf5GV"}},{"cell_type":"markdown","source":[],"metadata":{"id":"1nNE2ziahAIX"}},{"cell_type":"markdown","source":["**TEXT ANALYSIS**"],"metadata":{"id":"KhYM4c0Ldani"}},{"cell_type":"markdown","source":["OK, so now we've got some data.\n","\n","But what happens when we want to analyze this data? Well, we might just look at closely or read it, for ourselves - this would be easy, for example, if we just had one page of text to look at; but it would start to get hard if we had thousands.\n","\n","For this reason, we might want to use computational methods of text analysis. Today, we're going to learn how to use some of them with code."],"metadata":{"id":"L9AGWqdyI48_"}},{"cell_type":"markdown","source":["Please note however that we probably will not get through all of this today! So, we'll start this text analysis work in today's workshop, and then we'll move on to finishing it in the next"],"metadata":{"id":"vfIUj8d7g4sy"}},{"cell_type":"markdown","source":["First, let's grab some text to analyze. In honor of the internet, we're going to use the text of the taylor swift song, shake it off. I'm going to save that text below, in a string, called text.\n"],"metadata":{"id":"U9uxm--VJZwt"}},{"cell_type":"code","source":["text = \"I stay out too late Got nothing in my brain That's what people say That's what people say I go on too many dates But I can't make them stay At least that's what people say That's what people say But I keep cruising Can't stop, won't stop moving It's like I got this music in my mind Saying it's gonna be alright I never miss a beat I'm lightning on my feet And that's what they don't see That's what they don't see Players gonna play, play, play, play, play And the haters gonna hate, hate, hate, hate, hate (haters gonna hate) Baby, I'm just gonna shake, shake, shake, shake, shake I shake it off, I shake it off Heartbreakers gonna break Fakers gonna fake I'm just gonna shake I shake it off, I shake it off I shake it off, I shake it off I, I, I shake it off, I shake it off I, I, I shake it off, shake it off I, I, I shake it off, I shake it off I, I, I shake it off, I shake it off I, I, I shake it off, I shake it off I, I, I, shake it off, I shake it off I, I, I, shake it off, I shake it off\""],"metadata":{"id":"Nb1KSiH6Keol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OK, so there are ways to use Python to analyze text. But it's worth noting, before we do that, that there are now actually certain software programs that do some of these things for you, without your having to code it yourself.\n","\n","Just like there was a nice little app we used to scrape the reddit api for us, there's also nice apps for text analysis. let's look at one here:\n","\n","https://voyant-tools.org/\n","\n","\n","OK, now that we're back, let's learn how to do things like this with Python."],"metadata":{"id":"DI5knc3HK3E6"}},{"cell_type":"code","source":["text.lower()"],"metadata":{"id":"eOEExES9KyZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text.upper()"],"metadata":{"id":"OuxlXUuhNe-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text.count(\"shake\")"],"metadata":{"id":"w_oNBozzN0ZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Like the for loop above, we can iterate through sequences of the\n","# result of a method called split(). The split method is going to\n","# be *very important* for text analysis.\n","#\n","# The default way in which split() works is to split on the delimiter\n","# of a space--ideal for splitting a sentence. We'll worry more about\n","# what is returned by split() later, but this gets us started with\n","# working on words.\n","\n","for word in text.split():\n","    print(word)\n"],"metadata":{"id":"mIlUY4PpOEDf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's generate some basic data from this very long string\n","print(\"Total characters:\",len(text))\n","print(\"Total word count:\",len(text.split()))\n","print(\"Paragraph count:\",text.count(\"\\n\"))\n","print(\"Rough sentence count:\",text.count(\".\"))"],"metadata":{"id":"aAwBhQVvOOwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We now know how to process each word. Let's find long words.\n","# What is a long word?\n","long_word = 10\n","for word in text.split():\n","\n","    # This is our first *conditional statement*\n","    # the \"if\" means if a certain thing is true. if the statement that follows is true, then the code will perform the action on the next line\n","\n","    if len(word) >= 10:\n","        print(word)"],"metadata":{"id":"Z_WTr3-AOV3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's begin by loading up some important libraries/packages\n","import numpy as np\n","import glob as glob\n","import nltk\n","\n","#nltk in particular is very useful for text analysis, and the main package you'll often use\n","\n","from nltk.probability import FreqDist\n","from nltk.tokenize import RegexpTokenizer\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","# allow for displaying of graphics\n","%matplotlib inline"],"metadata":{"id":"rdQjLN8CPcl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["allwords=[]\n","words = tokenizer.tokenize(text)\n","for w in words:\n","    allwords.append(w)\n","\n","freqdist = FreqDist(allwords)\n","freqlist = freqdist.most_common()\n","print(freqlist)"],"metadata":{"id":"AZSmvHmKPqxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","sns.set_style('darkgrid')\n","freqdist.plot(20);"],"metadata":{"id":"KQOjrXTPPzHK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's run nltk part of speech tagger on the tokens:\n","\n","#normally, when we use the part of speech tagger on tokens, we do this. But notice colab is going to throw an error\n","#so let's practice reading error messages and dealing with them, what should we do?\n","pos = nltk.pos_tag(words)\n","print(pos)"],"metadata":{"id":"AfHzVVPxP9s_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#INSERT YOUR CODE HERE TO FIX THE PROBLEM\n","\n"],"metadata":{"id":"VsZMUkZ7QeWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ok, now let's continue\n","\n","# Now we run the tagger on the tokens:\n","pos = nltk.pos_tag(words)\n","print(pos)"],"metadata":{"id":"Wnc1UwZ1Qi6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nouns = []\n","for p in pos:\n","    if p[1] == 'NN':\n","        nouns.append(p[0])\n","nounfreq = FreqDist(nouns)\n","nounfreqlist = nounfreq.most_common()\n","\n","print(nounfreqlist)"],"metadata":{"id":"SUC0thw9Qqxg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\"Assignment 1\""],"metadata":{"id":"Zv4UcLqFjHvp"}},{"cell_type":"code","source":["#alright, now you have a coding assignment, which we're going to walk through together\n","#OR leave for next time if we need to\n","#I want us to scrape a wiki page and then do one piece of text analysis from above to it\n","#so, we want to walk through these steps..."],"metadata":{"id":"82j3WIO9QvUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#first, find the code from last week to scrape a wiki page and run it below, in blocks you create"],"metadata":{"id":"bbDHYIMTQ_5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#second, take the text of the wiki page and give it a name, like \"text1\"\n","#do this in the same way that we titled the 'text\" of the tay swift song in the code above. Create a variable called text1 and set it equal to a string of the wiki page text"],"metadata":{"id":"rW-KG3FkiruK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#do some type of analysis, from above, on the wiki page text, called 'text1\""],"metadata":{"id":"7Lxu8XLWi3JW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Assignment 2 (we'll start here next time, most likely)\n","\n","Analyze the reddit data we collected earlier\n","\n","first, the easy way: upload your reddit spreadsheet into voyant and let's see what we can do\n","(hint - you might first want to create a new spreadsheet file just with the columb of text you want to analyze)\n","\n","second, the hard way: upload the reddit spreadsheet into this coding notebook and analyze"],"metadata":{"id":"_TTOByGYjLA-"}},{"cell_type":"code","source":["#group activity, we'll work through this together"],"metadata":{"id":"Fg0MBtYDjs0J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Finally some more advanced text analysis with machine learning**\n","\n","I will likely update this code before we use it next week, so stay tuned..."],"metadata":{"id":"AELmKuDDjFEA"}},{"cell_type":"markdown","source":["OK, but as you know from reading my article, there are more complex things to do with texts, using machine learning; like topic modeling and classification. Next week, we're going to do both of those things, in addition to looking at a little image analysis. But, if we have time now, we can start topic modeling today....\n","\n","First, let's discuss what topic modeling is (return to real life); ok, we're back..."],"metadata":{"id":"FiCf7flpRCyl"}},{"cell_type":"markdown","source":["If you want to see a whole very basic workflow of topic modeling, the most basic steps, here is a tutorial: https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n","\n","this explains every block of this code, in case you want to topic model yourself. You'd have to substitue some of this for your data."],"metadata":{"id":"Rv1u6_OeUe7N"}},{"cell_type":"code","source":["doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n","doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n","doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n","doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n","doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n","\n","# compile documents\n","doc_complete = [doc1, doc2, doc3, doc4, doc5]"],"metadata":{"id":"mNeiArSYVQUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import string\n","\n","#I had to add this to their code, for colab not in original:\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","stop = set(stopwords.words('english'))\n","exclude = set(string.punctuation)\n","lemma = WordNetLemmatizer()\n","def clean(doc):\n","    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n","    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n","    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n","    return normalized\n","\n","doc_clean = [clean(doc).split() for doc in doc_complete]"],"metadata":{"id":"rE0F3mnmVWU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing Gensim\n","import gensim\n","from gensim import corpora\n","\n","# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n","\n","dictionary = corpora.Dictionary(doc_clean)\n","\n","# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n","doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"],"metadata":{"id":"Of-vXOiiVzMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating the object for LDA model using gensim library\n","Lda = gensim.models.ldamodel.LdaModel\n","\n","# Running and Trainign LDA model on the document term matrix.\n","ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"],"metadata":{"id":"m6707dP7WMcN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(ldamodel.print_topics(num_topics=3, num_words=3))"],"metadata":{"id":"zt1QbluNWSBV"},"execution_count":null,"outputs":[]}]}