{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRQ4z6TRjrn4UJLe9c4n+q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TessM2/content/blob/main/Workshop_3_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modeling**\n",
        "\n",
        "last week, we looked at some basic methods of text analysis with nltk (tokenization, wordcounts, part of speech tagging etc)\n",
        "\n",
        "but there are also more complicated (if more/equally limited) ways of engaging in textual, or imagistic, analysis\n",
        "\n",
        "A lot of these ways use machine learning models\n",
        "\n",
        "We've discussed machine learning, but let's briefly pause to discuss what a machine learning model is...\n"
      ],
      "metadata": {
        "id": "Pilp2zY9UcMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, now that we've done that...\n",
        "\n",
        "Today we're going to take a quick look at three types of text/image analysis models. \n",
        "\n",
        "As a caveat - each of these methods is questionable/problematic in its own way, and it's important to understand the pros/cons of each method. We'll definitely touch on this briefly, but I've also shared links where you can read about these methods in a bit more detail, if you are interested in using them\n",
        "\n",
        "As another caveat - this is truly the briefest introduction, and, as in all our coding work, we're only skimming the surface here. But it's meant as a starter kit, so that you can pursue any of these methods further if youre interested"
      ],
      "metadata": {
        "id": "BxLfsN0eVAGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic Modeling**\n",
        "\n",
        "This is a method of extracting the \"topics\" that appear in a set of textual documents. let's pause for a second to understand exactly what that means, in this context....\n",
        "\n",
        "Here, also are two links to more indepth explanations of what topic modeling is and how one might use it, for/from a more humanist perspective\n",
        "\n",
        "https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/\n",
        "https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html\n",
        "\n",
        "ok, let's look at some code for topic modeling..."
      ],
      "metadata": {
        "id": "1N1t0fgwVTJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the first thing we need to do is have some documents we want to work with; \n",
        "#here we'll make them ourselves, and use a very small sample\n",
        "#in reality, we'd gather the texts from a corpus of interest, and use a larger sample\n",
        "#though there are questions concerning the use of topic models on short texts (we can discuss, thouhg my tldr is it still works with lda even though other models might be preferable) you can try this on a group of tweets\n",
        "\n",
        "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
        "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
        "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
        "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
        "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
        "\n",
        "# compile documents\n",
        "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
      ],
      "metadata": {
        "id": "dkXHNwjtYJm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As human readers, what topics might we say that we see across the above documents?"
      ],
      "metadata": {
        "id": "Oddma8HTaMrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#next we need to clean and prepare our textual data; remember last time we discussed tokenization\n",
        "#here we're going to \"lemmatize\" and delete \"stopwords\", among other things; let's discuss what that means\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "#we need below for coding in colab, but would not otherwise:\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "\n",
        "#cleaning the text in each document in our list\n",
        "doc_clean = [clean(doc).split() for doc in doc_complete] "
      ],
      "metadata": {
        "id": "PKYUxVg8Zbg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One package we can use for topic modeling is gensim, though it's not the only one\n",
        "one type of topic model we can use is LDA though, again, it's not the only one\n",
        "see below on \"model selection\""
      ],
      "metadata": {
        "id": "kB0t97Mkakq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we're going to process the text, further, to prepare for the modeling\n",
        "here we'll put the features/tokens in a document term matrix\n",
        "let's talk for a moment about what that is"
      ],
      "metadata": {
        "id": "zWbQpXg7a8lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "\n",
        "# Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
        "\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. (in gensmin doc2bow will create the matrix of word quantities for us, on the dictionary object)\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
      ],
      "metadata": {
        "id": "6T7MKv5ZadOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model..."
      ],
      "metadata": {
        "id": "VlWU4zGtbJpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Trainign LDA model on the document term matrix.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
      ],
      "metadata": {
        "id": "OqHTDDs-ai6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing and examining the topics...thoughts?"
      ],
      "metadata": {
        "id": "hGIy5r_GbMiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ldamodel.print_topics(num_topics=3, num_words=3))"
      ],
      "metadata": {
        "id": "NlVVqTDGbIUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to highlight a few steps we just went through, in which we made a number of choices\n",
        "These are steps we go through with running/training almost every type of model\n",
        "And the choices we make in each of these steps effect the outcome\n",
        "They are\n",
        "\n",
        "1. preprocessing/cleaning (how do we process/prepare our text?)\n",
        "2. feature selection (what textual elements do we use as features?)\n",
        "3. Model selection (what type of model we use; here, lda; there are a few principles we might apply to choosing models, in general)\n",
        "4. parameter or hyperparameter tuning (the \"settings\" of the model, from how many topics we choose to create, passes through corpus during training)\n",
        "\n",
        "You have to consider all of these choices with any type of model"
      ],
      "metadata": {
        "id": "Se6rAJVpbUFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try rerunning the model above changing some of the parameters (e.g. number of topics of passes)"
      ],
      "metadata": {
        "id": "K-5nBOEOdE8J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1p6gZV_1dLWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6M1ORQqveOkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifier**\n",
        "\n",
        "next we can use a classifier\n",
        "these can work for both images and text\n",
        "we've discussed these before, but let's review, with the classic example of the spam filter...\n",
        "\n",
        "Here's an introduction to this method that also starts with the spam filter example\n",
        "\n",
        "https://developers.google.com/machine-learning/guides/text-classification\n",
        "\n",
        "Again, we're going to have to make choices through a number of steps. We're going to have to choose:\n",
        "1. how we pre-process the text\n",
        "2. what features we select to train the model on (lots of choices, in real classification)\n",
        "3. what type of model we use. we'll practice with naive bayes, but others are possible (why choose this one?)\n",
        "4. how we tunr parameters/hyperparameters. here, this matters quite a bit, as it will change the efficacy of the model by a good amount. especially the max-features parameter, as we'll see below\n",
        "\n"
      ],
      "metadata": {
        "id": "vNYcLSHJdMGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first, as always we need some data. basically, we need a corpus of two differnet categories of texts, in equal numbers, that we want to train our classifier to distinguish from one another\n",
        "\n",
        "in most classification problems the standard wisdom is that you want 1,000 samples of each type of text, which have to be labeled. labeling of texts into categories is often collective (via multiple annotators), crowdsourced (mechanical turk), or drawing from existing labels (like genre labels, on netflix). but you can also label yourself.\n",
        "\n",
        "Here's a sample corpus of two categories of texts, which are different types of medium posts that I labeled. The first group are texts of medium posts that are in what I call the \"7 habits\" style (let's look at these a second); the other group are texts of other randomly selected medium posts from self-help pubications. We're actually going to use just 585 of each.  \n",
        "\n",
        "To run a model, we want to start with a spreadsheet/matrix of the texts themselves, and then, associated with each, a label to indicate the type. we'll typically use the labels 1 and 0 for the two types. So, we want, with each of the 585 examples of the \"7 habits\" medium posts, the label 1, and with each of the 585 examples of the other posts, the label 0. \n",
        "\n",
        "Here we're going to import that data, and take a look at it\n",
        "\n",
        "please note that this method of importing data is something we only have to do on colab; in your own work, you'd import from your own machine; in colab, you'd use the method i put in the code from workshop 2 (I can show you again, if you need help)"
      ],
      "metadata": {
        "id": "err0ITs-e7ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, let's download the dataset we're going to use to train the model as a zipped archive\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://github.com/TessM2/7habitsdata/archive/refs/heads/main.zip\", \"dataset.zip\")\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"dataset.zip\", 'r') as zf:\n",
        "    zf.extractall()\n",
        "#Clean up after ourselves\n",
        "import os\n",
        "os.remove(\"dataset.zip\")\n",
        "sevendata = pd.read_csv('7habitsdata-main/sevenhabits585formodel.csv')\n",
        "sevendata"
      ],
      "metadata": {
        "id": "62q-KC0ahE_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the .head function also lets us view the first few rows of a dataframe\n",
        "print(sevendata.head())\n"
      ],
      "metadata": {
        "id": "KEQmkBz2kNjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next lets upload the packages we need\n",
        "#we can do a lot of machine learning modeling, as people often do, with a package called scikit learn\n",
        "#we're going to import scikit learn and some utilities/functions in that package we also might need\n",
        "#what do you think countvectorizer does?\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "YAESaL3PlOOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's try running a naive bayes classifier on the full texts (graphs, vs. titles)\n",
        "but first let's talk through some elements, here\n",
        "let's talk, especially, about splitting the data up into **training and testing sets**\n"
      ],
      "metadata": {
        "id": "3fKIdjkQltt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#vectorize the text and set max_features; this is our feature selection\n",
        "countvec = CountVectorizer(max_features = 6547)\n",
        "#below, you see another version of the above line we could use, hashed out, whic deletes stopwords\n",
        "#countvec = CountVectorizer(stop_words = \"english\", max_features = i)\n",
        "\n",
        "#vectorize the text (i.e., convert features to numerical values)\n",
        "vectors = countvec.fit_transform(sevendata['paragraphs'])\n",
        "labels = sevendata['7 habits']\n",
        "\n",
        "#initialize the model (take off shelf; multinomial naive bayes is the type we're using)\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "#fit the model to the data\n",
        "mnb.fit(vectors, labels)\n",
        "#run and get accuract scores, cross_validating ten different times\n",
        "#what do we mean by cross validating? *discuss\n",
        "\n",
        "scores = cross_val_score(mnb, vectors, labels, cv=10)\n",
        "scoreavg = (sum(scores) / len(scores))\n",
        "print(scoreavg)\n",
        "print(scores)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "ZLrlxzMdkix4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#note that setting parameters is very important, here; especially the max_features\n",
        "#what do you think max_features means?\n",
        "#try some other max_features numbers. does the accuracy come out better?\n"
      ],
      "metadata": {
        "id": "rZjwT1UJksAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#in reality, you might want to loop through a number of different max features to see where you get the highest accuracy\n",
        "#I already did this and determined that 6574 was the peak, at about 70.8 percent\n",
        "#but you could build your own loop, and set the max_features numbers you want t loop through, like this\n",
        "#set, in the range of the loop, the numbers of max features you want to loop through (don't make the range too big so it won't take too long;but you might try, e.g., a range of about 50 values)"
      ],
      "metadata": {
        "id": "zgi3dEF0lDpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting our code above in a loop, to look through ranges of max_features:\n",
        "\n",
        "#notice this takes a while, so you might want to stop it part way through\n",
        "\n",
        "for i in range (6500,6600):\n",
        "  countvec = CountVectorizer(max_features = i)\n",
        "  vectors = countvec.fit_transform(sevendata['paragraphs'])\n",
        "  labels = sevendata['7 habits']\n",
        "  mnb = MultinomialNB()\n",
        "  mnb.fit(vectors, labels)\n",
        "  scores = cross_val_score(mnb, vectors, labels, cv=10) \n",
        "  scoreavg = (sum(scores) / len(scores))\n",
        "  print(i)\n",
        "  print(scoreavg)\n",
        "  print(scores)"
      ],
      "metadata": {
        "id": "_qtEi-Fbmkov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after, we can look at the features the model \"relied\" on most to distinguish the two groups; \n",
        "#i.e., in this case, with simple word counts, we'll get this in the form of a proportion: how many times the word appeared in one type of text sv. in another\n",
        "#in these proportions we always add one to the numerator and denominator, though, to avoid dividing by zero\n",
        "\n",
        "#BEFORE WE DO THIS: make sure to rerun the model, once, with the parameters (max features) you want; this will run with that version of the model\n",
        "#you might just rerun the first version of the model before the loop, then skip the loop\n",
        "\n",
        "#getting top distinguishing words\n",
        "title_tokens = countvec.get_feature_names()\n",
        "\n",
        "mnb.feature_count_\n",
        "mnb.feature_count_.shape\n",
        "\n",
        "# number of times each token appears across all non7hab texts\n",
        "nonseven_token_count = mnb.feature_count_[0, :]\n",
        "nonseven_token_count\n",
        "\n",
        "# number of times each token appears across all 7hab texts\n",
        "seven_token_count = mnb.feature_count_[1, :]\n",
        "seven_token_count\n",
        "\n",
        "# create a DataFrame of tokens with their separate non7hab and 7hab counts\n",
        "tokens = pd.DataFrame({'token':title_tokens, 'nonseven':nonseven_token_count, 'seven':seven_token_count}).set_index('token')\n",
        "tokens.head()\n",
        "\n",
        "mnb.class_count_\n",
        "\n",
        "# add 1 to 7hab and non7hab counts to avoid dividing by 0\n",
        "tokens['nonseven'] = tokens.nonseven + 1\n",
        "tokens['seven'] = tokens.seven + 1\n",
        "tokens.sample(5, random_state=6)\n",
        "\n",
        "# calculate the ratio of 7hab-to-non7hab for each token\n",
        "tokens['seven_ratio'] = tokens.seven / tokens.nonseven\n",
        "tokens.sample(5, random_state=6)\n",
        "\n",
        "# examine the DataFrame sorted by ua_ratio\n",
        "# note: use sort() instead of sort_values() for pandas 0.16.2 and earlier\n",
        "tokens.sort_values('seven_ratio', ascending=False)\n"
      ],
      "metadata": {
        "id": "0t-IIGOdnBXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice again, we made lots of decisions here\n",
        "1. how we preprocessed text (kept stopwords)\n",
        "2. how we chose features (unigrams; could have chosen lots of other things, e.g. bigrams, trigrams, etc.)\n",
        "3. how we chose model (naive bayes, why?)\n",
        "4. how we chose parameters (max features)"
      ],
      "metadata": {
        "id": "Pu6TR_-coNwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try redoing the above code, but on the headline texts, instead of the full texts. what do you have to change in the code to redo that?\n",
        "\n",
        "copy and paste new code below, making the necessary changes"
      ],
      "metadata": {
        "id": "lyOXzX0woiIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kLuulYmJoqzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next time, I hope: image classification (no time today)"
      ],
      "metadata": {
        "id": "ApQNNHWeorHp"
      }
    }
  ]
}